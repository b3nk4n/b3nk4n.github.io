<!DOCTYPE html><html lang="en" data-mode="light" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Research" /><meta property="og:locale" content="en" /><meta name="description" content="Frame Prediction – Beyond a Video with Deep Learning" /><meta property="og:description" content="Frame Prediction – Beyond a Video with Deep Learning" /><link rel="canonical" href="https://b3nk4n.github.io/research/" /><meta property="og:url" content="https://b3nk4n.github.io/research/" /><meta property="og:site_name" content="Benjamin Kan" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-02-20T22:36:56+01:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Research" /><meta name="twitter:site" content="@b3nk4n" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-02-20T22:36:56+01:00","datePublished":"2025-02-20T22:36:56+01:00","description":"Frame Prediction – Beyond a Video with Deep Learning","headline":"Research","mainEntityOfPage":{"@type":"WebPage","@id":"https://b3nk4n.github.io/research/"},"url":"https://b3nk4n.github.io/research/"}</script><title>Research | Benjamin Kan</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Benjamin Kan"><meta name="application-name" content="Benjamin Kan"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/img/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Benjamin Kan</a></div><div class="site-subtitle font-italic">Software Engineering. Cloud Native. Machine Learning. Game Development. Biking. Hiking. Mindfulness. Cooking.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-user ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a><li class="nav-item"> <a href="/projects/" class="nav-link"> <i class="fa-fw fas fa-laptop-code ml-xl-3 mr-xl-3 unloaded"></i> <span>PROJECTS</span> </a><li class="nav-item active"> <a href="/research/" class="nav-link"> <i class="fa-fw fas fa-microscope ml-xl-3 mr-xl-3 unloaded"></i> <span>RESEARCH</span> </a><li class="nav-item"> <a href="/learning/" class="nav-link"> <i class="fa-fw fas fa-book ml-xl-3 mr-xl-3 unloaded"></i> <span>LEARNING LOG</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/privacy/" class="nav-link"> <i class="fa-fw fas fa-user-lock ml-xl-3 mr-xl-3 unloaded"></i> <span>PRIVACY POLICY</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <a href="https://github.com/b3nk4n" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://twitter.com/b3nk4n" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a> <a href="https://www.linkedin.com/in/bkan/" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href="https://stackoverflow.com/users/3053918/b3nk4n" aria-label="stack-overflow" target="_blank" rel="noopener"> <i class="fab fa-stack-overflow"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>Research</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Research</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip class="dynamic-title"> Research</h1><div class="post-content"><h2 id="frame-prediction--beyond-a-video-with-deep-learning"><span class="mr-2">Frame Prediction – Beyond a Video with Deep Learning</span><a href="#frame-prediction--beyond-a-video-with-deep-learning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>This research project has been developed in <i class="fab fa-python"></i> Python using <a href="https://www.tensorflow.org/">TensorFlow</a>, <a href="http://www.numpy.org/">NumPy</a> and my high-level framework <a href="https://github.com/b3nk4n/tensorlight">TensorLight</a>. The code is available as open source in my <a href="https://github.com/b3nk4n/imseq"><i class="fab fa-github"></i> GitHub/imseq</a> repository. Furthermore, in case you would like to read my <a href="/assets/docs/2016/msc_thesis_bsautermeister.pdf"><i class="fas fa-file-pdf"></i> Master’s thesis</a> in detail, then please feel free to check it out. Otherwise, the following sections should give you a brief summary of the most important bits.</p><h3 id="motivation"><span class="mr-2">Motivation</span><a href="#motivation" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>In recent years, the field of deep learning achieved considerable success and according to its underlying philosophy: <em>“if we have a reasonable end-to-end model and sufficient data for training it, we are close to solving the problem.”</em><sup id="fnref:shi"><a href="#fn:shi" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>. But while there has been a lot of studies and practical applications of object recognition on static images or speech recognition, the application of these concepts on video data are just about to make their first steps in research.</p><p>Early deep learning approaches dealing with video data or simple image sequences address problems like human action recognition or video classification. Another example is optical flow prediction in order to detect the visual flow from one frame to the next. Most of these approaches require lots of labeled data to be able to train a network. The effortful labeling process and thus the resulting low availability of such data might be the main reason why this topic has not been covered that well so far. On the contrary, online services like YouTube provide a seemingly endless, but unlabeled source of videos to learn from.</p><h3 id="problem-statement"><span class="mr-2">Problem Statement</span><a href="#problem-statement" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>Throughout this thesis, it is investigated whether deep learning techniques can be successfully applied on videos to learn a meaningful representation in a completely unsupervised fashion. In detail, it is examined if such a representation is suited to continue a video even after it has finished. Hence, to learn a notion of the spatial and temporal evolution within a sequence of images as well as to get an idea of motion and dynamics of a scene. Such a high-level understanding would be helpful for autonomous intelligent agents that have to act and therefore understand our environment including its physical and temporal constrains<sup id="fnref:srivastava"><a href="#fn:srivastava" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. Other application areas might be for instance video compression<sup id="fnref:ascenso"><a href="#fn:ascenso" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, visual systems for autonomous cars or as a replacement for optical flow in causal video segmentation<sup id="fnref:couprie"><a href="#fn:couprie" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>. Aside from that, other supervised learning tasks like human action recognition could benefit from such a pre-trained network in order to improve the overall performance or to reduce the training time. Needless to say, other forms of transfer learning are easily conceivable as well.</p><h3 id="network-architecture"><span class="mr-2">Network Architecture</span><a href="#network-architecture" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The network architecture used in the thesis is based on the recurrent encoder-decoder architecture<sup id="fnref:cho"><a href="#fn:cho" class="footnote" rel="footnote" role="doc-noteref">5</a></sup> <sup id="fnref:sutskever"><a href="#fn:sutskever" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>. A first encoder RNN sequentially processes the input data and therefore preserves the temporal correlations of the data. The second decoder RNN is then initialized with the hidden state of the first recurrent network and is then able to generate the future predictions one after another by unrolling the network.</p><p><img data-src="/assets/img/research/rnn_enc_dec.png" alt="RNN Encoder Decoder" data-proofer-ignore> <em>Basic structure of a conditional recurrent autoencoder. The inputs (green) are processed by the encoder RNN to learn the representation of the data in sequence. Then, the decoder RNN takes over to infer the reconstructions (blue) of the inputs in reverse order.</em></p><p>In order to preserve the spatial structure of the data within the recurrent network cells, convolutional <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">LSTM</a> cells<sup id="fnref:shi:1"><a href="#fn:shi" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> are used. Technically, all internal matrix multiplications are exchanged with convolution operations. As a consequence, the data in feature space that flows through the <em>ConvLSTM cells</em> has a 3D shape instead of being just a 1D vector.</p><p><img data-src="/assets/img/research/conv_lstm.png" alt="Conv2D LSTM Cell" data-proofer-ignore> <em>The simplified structure of the batch-normalized ConvLSTM cell including peephole connections. The inputs and previous hidden states are convolved to produce 3D tensors that flow through each cell. Changes to standard FC-LSTM are highlighted in red.</em></p><p>The ConvLSTM implementation shown above further takes advantage of peephole connections and internal batch normalizations<sup id="fnref:cooijmans"><a href="#fn:cooijmans" class="footnote" rel="footnote" role="doc-noteref">7</a></sup>. The use of the latter modifications did not show any improvements, but had a negative effect on the overall training performance; hence these have not been used in the final evaluation.</p><p>To improve the training process of the decoder RNN, a recurrent training strategy called <em>scheduled sampling</em><sup id="fnref:bengio"><a href="#fn:bengio" class="footnote" rel="footnote" role="doc-noteref">8</a></sup> is used. By using this strategy, the trained network always ended up in a better prediction performance.</p><p><img data-src="/assets/img/research/scheduled_sampling.png" alt="Scheduled Sampling" data-proofer-ignore> <em>Influences of scheduled sampling regarding the training and validation error in context of recurrent networks and future frames prediction on Moving MNIST.</em></p><p>The overall network architecture consists of 5 major components, namely spatial encoder, spatio-temporal encoder, spatio-temporal decoder, spatial decoder and perceptual motivated loss layer. The network was trained end-to-end.</p><p><img data-src="/assets/img/research/model_arch.png" alt="Model Architecture" data-proofer-ignore> <em>The ConvLSTM Encoder-Predictor Model. Weights of the convolutional encoder (green) and decoder (blue) are shared layer-wise across the whole sequence. Spatial encodings of the ground truth frames, scheduled sampling components (yellow) and the loss layer (red) are only used while training.</em></p><p>In case you would like to know more about the technical implementation details and the recurrent encoder-predictor model, please have a look at my <a href="/research/#frame-prediction--beyond-a-video-with-deep-learning">linked Master’s thesis</a> at the top of this page.</p><h3 id="evaluation-of-results"><span class="mr-2">Evaluation of Results</span><a href="#evaluation-of-results" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>The network model has been assessed using tree different datasets with increasing complexity. In all experiments, the network was trained for only 100,000 steps using a <em>2-layer ConvLSTM</em>, ADAM optimizer<sup id="fnref:adam"><a href="#fn:adam" class="footnote" rel="footnote" role="doc-noteref">9</a></sup> in its default settings but <code class="language-plaintext highlighter-rouge">η=0.0005</code>, <em>scheduled sampling</em> and <em>batch normalization</em> in each convolutional layer of the spatial encoder and decoder on a single <a href="https://www.nvidia.com/en-us/geforce/products/10series/titan-x-pascal/">Nvidia GTX Titan X GPU</a>. All shown samples are generated using the test set.</p><h4 id="moving-mnist-dataset"><span class="mr-2">Moving MNIST dataset</span><a href="#moving-mnist-dataset" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>The <a href="http://www.cs.toronto.edu/~nitish/unsupervised_video/">Moving MNIST</a> dataset has been used in the first experiment to explore the hyperparameters of the model. In this dataset, two handwritten digits bounce inside a black colored square at size <code class="language-plaintext highlighter-rouge">64 × 64</code>. Even that this dataset looks very simple, the network has to deal with a high degree of occlusion.</p><p><img data-src="/assets/img/research/mm-anim-2digits.gif" alt="Results on MNIST" data-proofer-ignore> <em>Frame prediction results of Moving MNIST with 2 digits using the proposed model (right) compared to the ground truth (left).</em></p><p>Even when the model instance is trained on samples with two digits only, it is also able to deal with samples having one or three digits as well, due to the fully-convolutional approach. In contrary, other existing approaches are dealing with a hallucination or merging effect.</p><p><img data-src="/assets/img/research/mm-anim-3digits.gif" alt="Results on MNIST" data-proofer-ignore> <em>Frame prediction results of Moving MNIST with 3 digits using the proposed model (right) compared to the ground truth (left).</em></p><h4 id="mspacman-dataset"><span class="mr-2">MsPacman dataset</span><a href="#mspacman-dataset" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>In a second experiment, we use the <a href="https://github.com/dyelax/Adversarial_Video_Generation">MsPacman</a> dataset that contains recordings of the classic video game. To successfully predict the future frames in this dataset, the network requires to encode the content and the dynamics happening in the random crop of size <code class="language-plaintext highlighter-rouge">32 × 32</code>. Additionally, the neural network has to understand the game rules and environment on a primitive level. Due to the fully-convolutional approach, it is able to do inference on the full-sized frame as well.</p><p><img data-src="/assets/img/research/pac-anim.gif" alt="Results on MsPacman" data-proofer-ignore> <em>Frame prediction results of MsPacman using the proposed model (right) compared to the ground truth (left).</em></p><h4 id="ucf-101-dataset"><span class="mr-2">UCF-101 dataset</span><a href="#ucf-101-dataset" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p>The last experiment uses the <a href="http://crcv.ucf.edu/data/UCF101.php">UCF-101</a> dataset without its labels for human action recognition. This is the most complex task, since the image space is not limited to specific colors, as well as the endless possibilities of e.g. motion or camera changes. In many generated examples, the effect of slowly disappearing foreground objects of fast moving objects can be observed. One possible reason for this might be that the kernel size of the <em>ConvLSTM cells</em>, which defines the maximum possible motion between two frames, has been chosen to be too low.</p><p><img data-src="/assets/img/research/ucf-anim.gif" alt="Results on UCF-101" data-proofer-ignore> <em>Frame prediction results of UCF-101 using the proposed model (right) compared to the ground truth (left).</em></p><h3 id="references"><span class="mr-2">References</span><a href="#references" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><div class="footnotes" role="doc-endnotes"><ol><li id="fn:shi"><p>Shi et al., 2015 – <a href="https://arxiv.org/abs/1506.04214">Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting</a> <a href="#fnref:shi" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:shi:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p><li id="fn:srivastava"><p>Srivastava et al., 2015 – <a href="https://arxiv.org/abs/1502.04681">Unsupervised Learning of Video Representations using LSTMs</a> <a href="#fnref:srivastava" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:ascenso"><p>Ascenso et al., 2005 – <a href="http://amalia.img.lx.it.pt/~fp/artigos/EURASIP05_DVC_final.pdf">Improving Frame Interplation with Spatial Motion Smoothing for Pixel Domain Distributed Video Coding</a> <a href="#fnref:ascenso" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:couprie"><p>Couprie et atl., 2013 – <a href="https://arxiv.org/abs/1301.1671">Causal graph-based video segmentation</a> <a href="#fnref:couprie" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:cho"><p>Cho et al., 2014 – <a href="https://arxiv.org/abs/1406.1078">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a> <a href="#fnref:cho" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:sutskever"><p>Sutskever et al., 2014 – <a href="https://arxiv.org/abs/1409.3215">Sequence to Sequence Learning with Neural Networks</a> <a href="#fnref:sutskever" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:cooijmans"><p>Cooijmans et al., 2016 – <a href="https://arxiv.org/abs/1603.09025">Recurrent Batch Normalization</a> <a href="#fnref:cooijmans" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:bengio"><p>Bengio et al., 2015 – <a href="https://arxiv.org/abs/1506.03099">Scheduled Sampling for Sequence Prediction with Recurrent Neural Networks</a> <a href="#fnref:bengio" class="reversefootnote" role="doc-backlink">&#8617;</a></p><li id="fn:adam"><p>Kingma et al., 2014 – <a href="https://arxiv.org/abs/1412.6980">Adam: A Method for Stochastic Optimization</a> <a href="#fnref:adam" class="reversefootnote" role="doc-backlink">&#8617;</a></p></ol></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"> <script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2025 <a href="https://github.com/b3nk4n">Benjamin Kan</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/windows-phone/">windows phone</a> <a class="post-tag" href="/tags/windows/">windows</a> <a class="post-tag" href="/tags/downloads/">downloads</a> <a class="post-tag" href="/tags/python/">python</a> <a class="post-tag" href="/tags/uwp/">uwp</a> <a class="post-tag" href="/tags/csharp/">csharp</a> <a class="post-tag" href="/tags/tensorflow/">tensorflow</a> <a class="post-tag" href="/tags/deep-learning/">deep learning</a> <a class="post-tag" href="/tags/game-dev/">game dev</a> <a class="post-tag" href="/tags/performance/">performance</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script defer src="/assets/js/dist/page.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-2V2DLEFQQ9"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-2V2DLEFQQ9'); }); </script>
